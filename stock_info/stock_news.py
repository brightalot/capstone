import requests
from flask import Flask, request, jsonify
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
from stock_info.draw_chart import code_by_name
# ëŒ€ìƒ URL
url = "https://finance.naver.com/news/news_list.naver?mode=RANK"

def get_finance_news():
    _url = url
    # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
    response = requests.get(_url)
    response.encoding = 'euc-kr'  # ë„¤ì´ë²„ ê¸ˆìœµ í˜ì´ì§€ëŠ” euc-kr ì¸ì½”ë”© ì‚¬ìš©
    html = response.text

    # BeautifulSoup ê°ì²´ ìƒì„±
    soup = BeautifulSoup(html, 'html.parser')

    # ë‰´ìŠ¤ ë¸”ë¡ ì°¾ê¸°
    news_list = soup.select('li.block1 ul.simpleNewsList > li')

    # ë‰´ìŠ¤ ì •ë³´ ì¶”ì¶œ
    news_data = []
    for news in news_list:
        title_tag = news.find('a')
        title = title_tag['title']
        link = "https://finance.naver.com/" + title_tag['href']
        # JSON í˜•íƒœë¡œ ë³€í™˜
        link_dict = {"web": "https://finance.naver.com/" + title_tag['href']}
        press = news.find('span', class_='press').get_text(strip=True)
        wdate = news.find('span', class_='wdate').get_text(strip=True)
        description = press + " | " + wdate
        print(link)
        print(title)
        news_data.append({
            "title": title,
            "description": description,
            "link": link_dict
        })
    return news_data[:5]


app = Flask(__name__)

# # ğŸ“° íŠ¹ì • ì¢…ëª© ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
# def get_stock_news(stock_name):
#     startdate = datetime.datetime.strptime('2024-02-01', "%Y-%m-%d")  # ê²€ìƒ‰ ì‹œì‘ ë‚ ì§œ
#     enddate = datetime.datetime.strptime('2024-02-08', "%Y-%m-%d")  # ê²€ìƒ‰ ì¢…ë£Œ ë‚ ì§œ
#     news_list = []

#     # Chrome ë“œë¼ì´ë²„ ì„¤ì •
#     serv = Service(ChromeDriverManager().install())
#     chrome_options = webdriver.ChromeOptions()
#     chrome_options.add_argument('--headless')  # GUI ì—†ì´ ì‹¤í–‰
#     chrome_options.add_argument('--no-sandbox')
#     chrome_options.add_argument('--disable-dev-shm-usage')
#     driver = webdriver.Chrome(service=serv, options=chrome_options)

#     try:
#         # ë‰´ìŠ¤ ê²€ìƒ‰ URL ìƒì„± (ë§¤ì¼ê²½ì œ ê¸°ì¤€)
#         startdate_str = startdate.strftime("%Y-%m-%d")
#         enddate_str = enddate.strftime("%Y-%m-%d")
#         url = f'https://www.mk.co.kr/search?word={stock_name}&dateType=direct&startDate={startdate_str}&endDate={enddate_str}&searchField=title'
#         driver.get(url)

#         # ë‰´ìŠ¤ ê°œìˆ˜ í™•ì¸
#         try:
#             n_news = WebDriverWait(driver, 10).until(
#                 EC.presence_of_element_located((By.XPATH, '/html/body/div[2]/main/section/div/div[2]/div/div/div[1]/section/header/div[1]/h2/span'))
#             ).text
#         except:
#             n_news = '0'

#         if n_news.isdigit() and int(n_news) > 0:
#             soup = bs(driver.page_source, 'html.parser')
#             news_date = soup.find_all("div", {"class": "txt_area"})

#             for n in news_date[:5]:  # ìµœì‹  5ê°œ ë‰´ìŠ¤ ì œê³µ
#                 date_text = n.find("div", {"class": "info_group"}).find("p", {"class": "time_info"}).text
#                 title_text = n.find("h3", {"class": "news_ttl"}).text
#                 news_list.append({"date": date_text, "title": title_text})

#     except Exception as e:
#         print("Error during news scraping:", e)

#     driver.quit()
#     return news_list


def get_stock_news(stock_code):
    iframe_url = f"https://finance.naver.com/item/news_news.naver?code={stock_code}&page=1"
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/99.0.4844.51 Safari/537.36"
        )
    }

    res = requests.get(iframe_url, headers=headers)
    res.encoding = "euc-kr"
    html = res.text

    # 1) í˜¹ì‹œë¼ë„ ë‚´ìš©ì´ ë¹„ì •ìƒì¸ì§€ í™•ì¸í•´ë³´ì„¸ìš”.
    # print(html)

    soup = BeautifulSoup(html, "html.parser")
    table = soup.select_one("table.type5")
    if not table:
        return []  # í…Œì´ë¸” ìì²´ê°€ ì—†ìœ¼ë©´ []

    rows = table.select("tbody > tr")

    news_data = []
    for row in rows:
        # relation ê´€ë ¨ í´ë˜ìŠ¤ëŠ” ê±´ë„ˆë›°ê¸°
        row_classes = row.get("class", [])
        if "relation_tit" in row_classes or "relation_lst" in row_classes:
            continue

        title_tag = row.select_one("td.title > a.tit")
        press_tag = row.select_one("td.info")
        date_tag = row.select_one("td.date")

        if title_tag and press_tag and date_tag:
            title = title_tag.get_text(strip=True)
            link = title_tag.get("href")
            press = press_tag.get_text(strip=True)
            wdate = date_tag.get_text(strip=True)
            news_data.append({
                "title": title,
                "description": f"{press} | {wdate}",
                "link": {"web": link}
            })
    print(news_data)
    return news_data